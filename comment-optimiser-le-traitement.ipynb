{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf0546d-506a-4ae6-a569-eabe58a2205b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Comment réduire la durée du traitement\n",
    ">Il existe plusieurs techniques et méthodes pour réduire la durée du traitement.\n",
    "Voici les principales:\n",
    "- Utiliser une librairie de traitement de données qui est optimisée pour le traitement parallèle (pandas vs pyspark).\n",
    "- Nettoyer le jeux de données (data cleaning).\n",
    "- Choisir un format de fichier plus perfomant (.csv vs .parquet)\n",
    "- Utiliser un cluster plus puissant ou mieux adapté au type de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfba45b7-04c6-4ed3-ad01-a05b2d98c51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Utiliser la librairie pyspark ou pyspark.pandas\n",
    ">Les librairie pyspark et pyspark.pandas permettent le traitement sur plusieurs ordinateurs:\n",
    "- Ceci permet, entre autre, déviter les dépassements en mémoire, dans le cas ou le fichier a traiter serait très gros.\n",
    "- Accèlere le traitement car plusieurs ordinateur vont traiter en même temps chacun une partie distincte du fichier.\n",
    "***\n",
    "[NOTE] Pour des perfomance optimal, ne pas oublier de:\n",
    "- Désactiver le mode ANSI pour PySpark.Pandas\n",
    "- Activer le mode ARROW\n",
    "voir exemple ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c91dac-72bb-493b-a0f7-187e23505551",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemple"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession #utilisez pour configurer certaines optimisations\n",
    "import pyspark.pandas as pd #Bon car le traitement va se faire en parallele sur plusieurs ordinateur\n",
    "import pandas #Moins bon car le traitement va se faire sur un seul ordinateur\n",
    "\n",
    "#Optimisation\n",
    "if spark is None:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"False\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e6f7b9d-83bd-4818-978f-ffded9073b00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Élément important à considérer lors de la conversion de Pandas vers PySpark.Pandas.\n",
    ">Il se peut que quelque fonctionalitées de Pandas ne soit pas disponible pour Pyspark.Pandas. Ceci est normal car Pandas à été designé pour fonctionner sur un seul ordinateur alors que PySpark.Pandas a été designer pour fonctionner de facon distribué sur plusieurs ordinateurs. En conséquence, les DataFrames de Pandas sont mutable (peuvent être modifié) et ceux de PySpark/PySpark.Pandas sont immutable (ne peuvent pas être modifié).\n",
    "\n",
    "ref: https://spark.apache.org/docs/latest/api/python/tutorial/pandas_on_spark/supported_pandas_api.html\n",
    "\n",
    "Voir les exemples ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc455900-14ea-402a-b9e1-fe0893fac190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Modifier un DataFrame Pandas partie 1\n",
    ">Le DataFrame de Pandas est mutable, ce qui lui permet de faire ses modifications directement sur le DataFrame.\n",
    "\n",
    "Voir exemple ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33354ad6-8e8d-4430-a1a6-596c6cf28b87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemple"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df.drop('A', axis=1, inplace=True) #modification directement fait dans le dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e49c2d-6bfc-45a6-a82c-939ac1af2c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Modifier un DataFrame Pandas partie 2\n",
    ">Pandas, peut aussi renvoyer un nouveau DataFrame pour ses transformations, Pandas est généralement utilisé de cette façon, pour les raisons suivantes:\n",
    "- Permet de conserver et réutilisez le DataFrame d'origine.\n",
    "- Ajout de la clareté au code.\n",
    "\n",
    "Voir exemple ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14eb5aa7-81e9-4ef5-86de-9ca5d03068f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemple"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = df.drop('A', axis=1) #modification fait en renvoyant un nouveau dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ea549ca-e3c5-4976-a23f-03c868a6949a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Modifier un DataFrame PySpark.Pandas\n",
    ">Le DataFrame de PySpark.Pandas est immutable, donc seul les modifications fait en renvoyant un nouveau dataframe sont valide\n",
    "\n",
    "Voir exemple ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba812e21-b267-455d-a44b-704663efc6ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemple"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = df.drop('A', axis=1) #modification fait en renvoyant un nouveau dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba5cef6-c3c7-4ad2-a600-9c1b2da385ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Executer PyPspark.Pandas avec le paramètre inplace=True\n",
    ">Si vous tenter d'executer une cellule avec une modification inplace, pour PySpark.Pandas, vous risquer de recevoir ce message d'erreur: *TypeError: DataFrame.drop() got an unexpected keyword argument 'inplace'*\n",
    "\n",
    "Voir exemple ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac64a634-492b-489a-ad6d-c9fae01348e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemple"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df.drop('A', axis=1, inplace=True) #Erreur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb780d68-40b8-45df-a31c-820db3b16336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Quoi faire si je dois utiliser une fonction qui existe seulement sur Pandas\n",
    ">Il est quand même possible d'appelez une fonction qui existe seulement dans Pandas en utilisant la fonction de conversion to_pandas et from_pandas.\n",
    "- to_pandas, rammenez les données dans un seul ordinateur ce qui rend le DataFrame mutable.\n",
    "- from_pandas, distribut les données as travers plusieurs ordinateur, ce qui rend le dataframe immutable.\n",
    "- Coût en perfomance élévé, donc as éviter\n",
    "\n",
    "Voir exemple ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48813c57-8603-4eb0-a95c-671c66138296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as pspd\n",
    "\n",
    "#Chargement du DataFrame PySpark.Pandas (immutable)\n",
    "psdf = pspd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "#Conversion de PySpark.Pandas en DataFrame Pandas (mutable)\n",
    "df = psdf.to_pandas()\n",
    "df.drop('A', axis=1, inplace=True)\n",
    "\n",
    "#Reconvertir en PySpark.Pandas\n",
    "psdf = pspd.from_pandas(df)\n",
    "\n",
    "#Remarquer que la colone a bel et bien été retiré\n",
    "display(psdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19429c89-577b-4237-84f0-751d58b9b2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Nettoyer le jeux de données\n",
    ">Le but du nettoyage de données est d'éliminer les données qui ne seront pas utilisé dans le cadre du calcul actuariel. \n",
    "- Permet de diminuer le temps de chargement.\n",
    "- Permet de réduire le temps des requêtes.\n",
    "***\n",
    "Pour l'exemple ci-dessous:\n",
    "- Nous ramenons seulement les colones requisent pour le calcule (FMID,MarketName, Zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef17a8c-4359-4e33-9a6d-31c7e0ea55d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as pd\n",
    "\n",
    "path = \"dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/data-001/market_data.csv\"\n",
    "df = pd.read_csv(path,usecols=['FMID','MarketName','zip'])\n",
    "\n",
    "print(f\"Nombre de colones {df.columns.size}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab0edce9-0ee6-4d84-9d92-01b88518ed80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Choisir et favoriser le format de fichier .parquet\n",
    ">Apache Parquet est un format de fichier en colonnes qui fournit des optimisations pour accélérer les requêtes. Il s'agit d'un format de fichier plus efficace que CSV ou JSON.\n",
    "- Optimisé pour storer de large quantité de données. Les fichiers parquet sont au dessus de 10 fois plus léger que l'équivalent en .CSV ou .JSON\n",
    "- Accélère massivement le temps de traitement en comparaison d'un fichier .CSV ou .JSON.\n",
    "- Réduit considérablement les côuts de traitement.\n",
    "***\n",
    "Voici un tableau de comparaison entre .parquet et .CSV\n",
    "| Type de fichier | Poids | Temps de réponse (query) | Qty de données scanné par la query |  Couts |\n",
    "|-----------------|-------|--------------------------|------------------------------------|--------|\n",
    "| .CSV | 1TB | 236 seconds | 1.15TB | 5.75$|\n",
    "| .parquet | 130GB | 6.78 seconds | 2.51GB | 0.01$ |\n",
    "| Gains | 87% plus petit | 34x rapide | 99% moins de traitement | 99.7% d'économie |\n",
    "\n",
    "Comme le tableau le démontre, le format .parqet est massivement plus perfomant et économique. Il y a donc aucune raison logique d'utiliser un fichier .CSV ou JSON pour le traitement de vos données.\n",
    "\n",
    "Nous allons vous montrer dans la cellule plus bas:\n",
    "- Comment charger un fichier .parquet\n",
    "- Comment convertir votre fichier .csv en fichier .parquet. \n",
    "\n",
    "La même logique s'applique as tout les autres formats de fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8196f0a9-880a-4ada-bf6e-4fa47db48345",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exemple"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Optimisation\n",
    "if spark is None:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"False\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "\n",
    "#Variables\n",
    "chemain_fichier_csv = \"/Volumes/alpha_cloud_ai_workspace/default/exp-data/hour.csv\"\n",
    "destination_fichier_parquet = \"/Volumes/alpha_cloud_ai_workspace/default/exp-data/hour.parquet\"\n",
    "\n",
    "#Lecture du CSV et Conversion en .parquet\n",
    "df = pd.read_csv(chemain_fichier_csv)\n",
    "df.to_parquet(destination_fichier_parquet)\n",
    "\n",
    "#Rechargement du fichier en parquet\n",
    "df = pd.read_parquet(destination_fichier_parquet)\n",
    "\n",
    "#Faire votre traitement & Calcul\n",
    "display(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "comment-optimiser-le-traitement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
