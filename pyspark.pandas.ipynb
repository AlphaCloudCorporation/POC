{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39af91e-484e-4518-963c-42dd3e75f122",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758297609693}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark as ps\n",
    "import pyspark.pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "builder = SparkSession.builder\n",
    "builder.config(\"spark.sql.ansi.enabled\", \"false\")\n",
    "builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "#builder.config('spark.executor.memory', '8g')\n",
    "#builder.config('spark.driver.memory', '8g')\n",
    "#builder.config(\"spark.driver.maxResultSize\", \"0\")\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "#Chargement\n",
    "df = pd.read_csv(\"/Volumes/alpha_cloud_ai_workspace/default/exp-data/cf_asset_mort_init.csv\", sep=\";\")\n",
    "\n",
    "#Ajout Interet\n",
    "df['tx_interet'] = df['taux_hyp'] / df['frequence']\n",
    "\n",
    "#Ajout des 1200 Cashflow\n",
    "colones = dict()\n",
    "for i in range(1200):\n",
    "    colones[f'cash_flow_{i+1}'] = 0\n",
    "df = df.reindex(columns=list(df.columns) + list(colones.keys()), fill_value=0)\n",
    "\n",
    "display(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59685f7-0215-4631-b98b-0f367141979f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758310015636}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Operations\n",
    "max_maturity = df[\"maturity\"].max()\n",
    "df = df.assign(cash_flow_1=lambda x: x.cash_flow_1 + x[\"versement\"])\n",
    "\n",
    "for i in range(2,max_maturity):\n",
    "    df = df.assign(**{f\"cash_flow_{i}\": lambda x, i=i: x[f\"cash_flow_{i-1}\"] - x[\"versement\"] + x[f\"cash_flow_{i-1}\"] * x[\"tx_interet\"]})\n",
    "\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3922c6ac-472a-4ca4-8c1a-993f8cd0edf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#25-06-30\n",
    "def projection_cf_mortgage(data):\n",
    " \n",
    "    # Initialize the DataFrame\n",
    "    cfs = np.zeros((len(data), 1200))\n",
    "   \n",
    "    # Iterate over each row in the data\n",
    "    for j in range(len(data)):\n",
    "        if data.iloc[j]['maturity']>0:\n",
    "            tx_interet = data.iloc[j]['taux_hyp'] / data.iloc[j]['frequence']\n",
    "            maturity = data.iloc[j]['maturity']\n",
    "            pay_cap = data.iloc[j]['versement']\n",
    "            notionnel_cul = np.zeros(maturity + 1)\n",
    "            notionnel_cul[0] = data.iloc[j]['solde_capital']\n",
    "           \n",
    "            for k in range(maturity):\n",
    "                notionnel_cul[k + 1] = notionnel_cul[k] - pay_cap + notionnel_cul[k]*tx_interet\n",
    "                cfs[j, k] += pay_cap\n",
    " \n",
    "            cfs[j, maturity - 1] += notionnel_cul[maturity]\n",
    " \n",
    "    # Convert the NumPy array to a DataFrame\n",
    "    cfs_df = pd.DataFrame(cfs, columns=[f'Cash Flow {i}' for i in range(1, cfs.shape[1] + 1)])\n",
    "   \n",
    "    # Concatenate the data with the cash flows\n",
    "    result = pd.concat([data.reset_index(drop=True), cfs_df], axis=1)\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark.pandas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
